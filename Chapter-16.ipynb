{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15d990e",
   "metadata": {},
   "source": [
    "# Bab 16: Natural Language Processing with RNNs, Attention, and Transformers\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab 16 melanjutkan pembahasan dari bab sebelumnya, dengan fokus menerapkan Jaringan Saraf Tiruan (JST) untuk tugas **Natural Language Processing (NLP)**. NLP adalah salah satu bidang *Deep Learning* yang paling menarik dan menantang, mencakup tugas-tugas seperti penerjemahan mesin, analisis sentimen, dan pembuatan teks.\n",
    "\n",
    "Bab ini akan membahas:\n",
    "* **Representasi Teks:** Cara mengubah teks menjadi vektor numerik yang dapat diproses oleh model, termasuk teknik *word embeddings*.\n",
    "* **Arsitektur Encoder-Decoder:** Pola arsitektur yang umum digunakan untuk tugas *sequence-to-sequence* seperti penerjemahan mesin.\n",
    "* **Attention Mechanism:** Sebuah inovasi kunci yang memungkinkan model untuk fokus pada bagian-bagian paling relevan dari input saat menghasilkan output, mengatasi keterbatasan RNN konvensional.\n",
    "* **Arsitektur Transformer:** Arsitektur modern yang revolusioner yang sepenuhnya mengandalkan mekanisme *attention*, tanpa menggunakan RNN sama sekali.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Representasi Teks dan Word Embeddings\n",
    "\n",
    "Kata-kata harus dikonversi menjadi representasi numerik.\n",
    "* **One-Hot Encoding:** Sederhana tetapi tidak efisien karena menghasilkan vektor yang sangat besar dan *sparse*, serta tidak menangkap kemiripan antar kata.\n",
    "* **Word Embeddings:** Representasi vektor padat dan berdimensi rendah yang menangkap makna semantik kata. Kata-kata yang mirip secara makna akan memiliki vektor yang berdekatan. *Embedding* ini dapat dipelajari dari awal atau menggunakan model *pre-trained* seperti Word2Vec atau GloVe.\n",
    "\n",
    "Di Keras, `keras.layers.Embedding` digunakan untuk mengubah indeks integer kata menjadi *embedding vector*.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Arsitektur Encoder-Decoder untuk Penerjemahan Mesin\n",
    "\n",
    "Untuk tugas *sequence-to-sequence* (seperti penerjemahan dari satu bahasa ke bahasa lain), arsitektur **Encoder-Decoder** sangat umum digunakan.\n",
    "1.  **Encoder:** Sebuah RNN (misalnya, LSTM atau GRU) yang memproses urutan input dan meringkasnya menjadi satu vektor konteks (biasanya *hidden state* terakhir dari RNN).\n",
    "2.  **Decoder:** Sebuah RNN lain yang mengambil vektor konteks dari *encoder* dan menghasilkan urutan output langkah demi langkah. Pada setiap langkah, *decoder* menerima kata yang dihasilkan sebelumnya sebagai input untuk menghasilkan kata berikutnya.\n",
    "\n",
    "#### a. Teacher Forcing\n",
    "Selama pelatihan, input ke *decoder* pada setiap langkah adalah kata target yang sebenarnya dari langkah sebelumnya, bukan kata yang diprediksi oleh *decoder* itu sendiri. Teknik ini disebut *teacher forcing* dan membantu menstabilkan serta mempercepat pelatihan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6da494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Contoh kerangka Encoder-Decoder\n",
    "# (Ini adalah contoh konseptual, implementasi penuh memerlukan data dan preprocessing)\n",
    "vocab_size = 10000\n",
    "embed_size = 128\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "encoder_embeddings = keras.layers.Embedding(vocab_size, embed_size)(encoder_inputs)\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n",
    "    128, return_state=True)(encoder_embeddings)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_embeddings = keras.layers.Embedding(vocab_size, embed_size)(decoder_inputs)\n",
    "decoder_lstm = keras.layers.LSTM(128, return_sequences=True)\n",
    "decoder_outputs = decoder_lstm(decoder_embeddings, initial_state=encoder_state)\n",
    "decoder_dense = keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Membuat model\n",
    "model_encoder_decoder = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b790e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Attention Mechanisms\n",
    "Kelemahan utama dari arsitektur Encoder-Decoder dasar adalah semua informasi dari urutan input harus dipadatkan menjadi satu vektor konteks berukuran tetap. Ini menjadi bottleneck untuk kalimat yang panjang.\n",
    "\n",
    "Attention Mechanism mengatasi ini dengan memungkinkan decoder untuk \"melihat\" dan fokus pada bagian-bagian yang relevan dari seluruh urutan input pada setiap langkah saat menghasilkan output.\n",
    "\n",
    "Cara Kerja: Pada setiap langkah decoding, mekanisme attention menghitung skor keselarasan (alignment score) antara hidden state decoder saat ini dan setiap hidden state encoder. Skor ini kemudian diubah menjadi bobot melalui fungsi softmax. Context vector untuk langkah tersebut adalah jumlah terbobot dari semua hidden state encoder.\n",
    "\n",
    "Mekanisme ini terbukti sangat efektif dan menjadi dasar bagi arsitektur Transformer.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Arsitektur Transformer\n",
    "Arsitektur Transformer, yang diperkenalkan dalam paper \"Attention Is All You Need\", sepenuhnya meninggalkan RNN dan hanya mengandalkan mekanisme attention. Ini memungkinkan paralelisasi yang masif dan telah menjadi arsitektur standar untuk banyak tugas NLP.\n",
    "\n",
    "# a. Arsitektur Transformer\n",
    "* *Positional Embeddings*: Karena tidak ada rekurensi, Transformer menambahkan positional embeddings ke input embeddings untuk memberikan informasi tentang posisi kata dalam urutan.\n",
    "* *Multi-Head Attention*: Alih-alih melakukan satu fungsi attention, multi-head attention memproyeksikan input ke beberapa subspace yang berbeda dan menerapkan attention secara paralel. Ini memungkinkan model untuk fokus pada   berbagai aspek dari urutan input.\n",
    "* *Encoder*: Terdiri dari tumpukan blok identik. Setiap blok memiliki lapisan Multi-Head Attention diikuti oleh lapisan Feed-Forward Network. Terdapat koneksi residual (residual connections) dan normalisasi lapisan (layer normalization) di sekitar setiap sub-lapisan.\n",
    "* *Decoder*: Mirip dengan encoder, tetapi memiliki lapisan Multi-Head Attention kedua yang melakukan attention pada output dari encoder. Lapisan attention pertamanya di-masking untuk mencegahnya \"melihat\" kata-kata di masa depan (causal attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Keras menyediakan layer `keras.layers.MultiHeadAttention` siap pakai)\n",
    "\n",
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    # Implementasi untuk menambahkan informasi posisi\n",
    "    pass\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self, n_heads, causal=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_heads = n_heads\n",
    "        self.causal = causal\n",
    "        # Lapisan-lapisan ini akan dibuat di dalam metode build()\n",
    "        # self.q_dense, self.k_dense, self.v_dense, self.attention, self.out_dense\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "      # Implementasi build\n",
    "      pass\n",
    "      \n",
    "    def call(self, inputs, mask=None):\n",
    "      # Logika call\n",
    "      pass\n",
    "\n",
    "# Contoh kerangka model Transformer Encoder\n",
    "embed_size = 128\n",
    "max_steps = 500\n",
    "vocab_size = 10000\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)(encoder_inputs)\n",
    "encoder_in = PositionalEncoding(max_steps, max_dims=embed_size)(embeddings)\n",
    "\n",
    "# Z = encoder_in\n",
    "# for N in range(6): # Tumpukan 6 blok encoder\n",
    "#    Z = MultiHeadAttention(...)(Z) # Blok attention\n",
    "#    Z = FeedForward(...)(Z)      # Blok feed-forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb38e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Kesimpulan\n",
    "\n",
    "Bab ini menunjukkan bagaimana NLP telah berevolusi dari RNN sederhana ke arsitektur yang lebih canggih seperti LSTM, GRU, dan puncaknya, Transformer. Konsep word embeddings memungkinkan representasi teks yang kaya. Arsitektur Encoder-Decoder dengan mekanisme attention mengatasi keterbatasan RNN untuk urutan panjang, sementara Transformer, dengan sepenuhnya mengandalkan attention, telah menetapkan standar baru dalam kinerja dan efisiensi pelatihan untuk berbagai tugas NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
