{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a39d1f",
   "metadata": {},
   "source": [
    "# Bab 4: Melatih Model (Training Models)\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab 4 ini beranjak dari pengenalan model *machine learning* sebagai \"kotak hitam\" dan mulai menyelami mekanisme internal serta algoritma pelatihan. Pemahaman mendalam tentang cara kerja model ini penting untuk memilih algoritma yang tepat, menyetel *hyperparameter*, melakukan *debugging*, dan analisis kesalahan secara efisien. Konsep-konsep yang dibahas dalam bab ini juga merupakan dasar penting untuk memahami *neural network*.\n",
    "\n",
    "Bab ini secara khusus membahas:\n",
    "* Model Regresi Linier dan dua metode pelatihannya: *Normal Equation* (solusi *closed-form*) dan *Gradient Descent* (pendekatan iteratif).\n",
    "* Regresi Polinomial untuk data non-linier, termasuk deteksi *overfitting* menggunakan *learning curves* dan teknik regularisasi.\n",
    "* Dua model klasifikasi: Regresi Logistik dan Regresi Softmax.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Regresi Linier (Linear Regression)\n",
    "\n",
    "Model regresi linier membuat prediksi dengan menghitung jumlah bobot (*weighted sum*) dari fitur-fitur input, ditambah dengan istilah bias (*bias term*).\n",
    "\n",
    "Rumus model Regresi Linier adalah sebagai berikut:\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$$\n",
    "Dalam bentuk vektor, rumus ini menjadi:\n",
    "$$\\hat{y} = h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\boldsymbol{\\theta} \\cdot \\mathbf{x}$$\n",
    "di mana:\n",
    "* $\\hat{y}$ adalah nilai yang diprediksi.\n",
    "* $n$ adalah jumlah fitur.\n",
    "* $x_i$ adalah nilai fitur ke-$i$.\n",
    "* $\\theta_j$ adalah parameter model ke-$j$ (termasuk istilah bias $\\theta_0$ dan bobot fitur $\\theta_1, \\theta_2, \\ldots, \\theta_n$).\n",
    "* $\\boldsymbol{\\theta}$ adalah vektor parameter model.\n",
    "* $\\mathbf{x}$ adalah vektor fitur *instance* (dengan $x_0$ selalu sama dengan 1).\n",
    "\n",
    "Fungsi biaya MSE untuk model Regresi Linier adalah:\n",
    "$$\\text{MSE}(\\mathbf{X}, h_{\\boldsymbol{\\theta}}) = \\frac{1}{m} \\sum_{i=1}^{m} (\\boldsymbol{\\theta}^\\intercal \\mathbf{x}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "#### a. The Normal Equation\n",
    "*Normal Equation* adalah solusi matematis (*closed-form*) yang secara langsung menghitung nilai $\\boldsymbol{\\theta}$ yang meminimalkan fungsi biaya.\n",
    "$$\\boldsymbol{\\theta} = (\\mathbf{X}^\\intercal \\mathbf{X})^{-1} \\mathbf{X}^\\intercal \\mathbf{y}$$\n",
    "\n",
    "Berikut adalah implementasi menggunakan NumPy dan Scikit-Learn.\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Membuat data linier acak\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Menambahkan x0 = 1 ke setiap instance (untuk bias term)\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Menghitung theta terbaik menggunakan Normal Equation\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "print(\"Theta terbaik yang dihitung dengan Normal Equation:\")\n",
    "print(theta_best)\n",
    "\n",
    "# Membuat prediksi menggunakan theta yang ditemukan\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "\n",
    "print(\"\\nPrediksi untuk X_new:\")\n",
    "print(y_predict)\n",
    "\n",
    "# Plot data dan garis regresi\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Prediksi\")\n",
    "plt.xlabel(\"<span class=\"math-inline\">x\\_1</span>\")\n",
    "plt.ylabel(\"<span class=\"math-inline\">y</span>\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.legend()\n",
    "plt.title(\"Regresi Linier dengan Normal Equation\")\n",
    "plt.show()\n",
    "\n",
    "# Melakukan hal yang sama dengan Scikit-Learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "print(\"\\nScikit-Learn intercept dan coefficient:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "```\n",
    "\n",
    "### b. Computational Complexity\n",
    "\n",
    "* Normal Equation: Memiliki kompleksitas komputasi sekitar $O(n^{2.4})$ hingga $O(n^3)$. Ini menjadi sangat lambat ketika jumlah fitur ($n$) sangat besar.\n",
    "* SVD (Scikit-Learn's LinearRegression): Sekitar $O(n^2)$.\n",
    "* Prediksi sangat cepat setelah model dilatih.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Gradient Descent\n",
    "\n",
    "Gradient Descent (GD) adalah algoritma optimisasi yang secara iteratif menyesuaikan parameter untuk meminimalkan fungsi biaya.\n",
    "* Learning Rate (Î·): Menentukan ukuran langkah. Jika terlalu kecil, konvergensi akan lambat. Jika terlalu besar, bisa divergen.\n",
    "* Feature Scaling: Penting untuk memastikan semua fitur memiliki skala yang serupa agar konvergensi lebih cepat.\n",
    "\n",
    "Langkah Gradient Descent:\n",
    "\n",
    "$$\\boldsymbol{\\theta}{next step} = \\boldsymbol{\\theta} - \\eta \\nabla{\\boldsymbol{\\theta}} \\text{MSE}(\\boldsymbol{\\theta})$$\n",
    "\n",
    "### a. Batch Gradient Descent\n",
    "\n",
    "Algoritma ini menggunakan seluruh training set untuk menghitung gradient pada setiap langkah.\n",
    "\n",
    "```python\n",
    "eta = 0.1  # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100 # jumlah instance\n",
    "\n",
    "theta = np.random.randn(2, 1)  # inisialisasi acak\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "print(\"Theta terbaik yang dihitung dengan Batch Gradient Descent:\")\n",
    "print(theta)\n",
    "```\n",
    "\n",
    "### b. Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent (SGD) mengambil satu instance acak pada setiap langkah. Ini jauh lebih cepat tetapi hasilnya \"memantul\" di sekitar minimum. Untuk mengatasinya, learning rate dikurangi secara bertahap (learning schedule).\n",
    "\n",
    "### c. Mini-batch Gradient Descent\n",
    "\n",
    "Algoritma ini menghitung gradient pada kelompok kecil (mini-batches). Ini menawarkan kompromi antara kecepatan SGD dan kestabilan Batch GD.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Regresi Polinomial (Polynomial Regression)\n",
    "\n",
    "Memungkinkan model linier untuk menyesuaikan data non-linier dengan menambahkan pangkat dari setiap fitur sebagai fitur baru.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Membuat data non-linier\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "# Menambahkan fitur polinomial (derajat 2)\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Melatih model Linear Regression pada data yang sudah ditransformasi\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "\n",
    "print(\"Intercept dan Coefficient dari Regresi Polinomial:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "# Plot hasil\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Prediksi\")\n",
    "plt.xlabel(\"<span class=\"math-inline\">x\\_1</span>\")\n",
    "plt.ylabel(\"<span class=\"math-inline\">y</span>\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.title(\"Regresi Polinomial Derajat 2\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### a. Learning Curves\n",
    "\n",
    "Learning curves adalah plot kinerja model pada training set dan validation set sebagai fungsi dari ukuran training set. Ini membantu mendiagnosis overfitting atau underfitting.\n",
    "* Underfitting: Kedua kurva mencapai plateau, berdekatan, dan memiliki error yang tinggi.\n",
    "* Overfitting: Error pada data pelatihan jauh lebih rendah daripada data validasi, dan ada celah di antara kurva.\n",
    "\n",
    "### b. The Bias/Variance Trade-off\n",
    "Error generalisasi model dapat dipecah menjadi:\n",
    "* Bias: Disebabkan oleh asumsi yang salah (cenderung underfit).\n",
    "* Variance: Disebabkan oleh sensitivitas berlebih terhadap data pelatihan (cenderung overfit).\n",
    "* Irreducible Error: Disebabkan oleh noise pada data.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Model Linier yang Diregularisasi (Regularized Linear Models)\n",
    "\n",
    "Regularisasi adalah cara untuk mengurangi overfitting dengan membatasi bobot model.\n",
    "\n",
    "### a. Ridge Regression\n",
    "\n",
    "Menambahkan istilah regularisasi $\\ell_2$ ($\\alpha \\sum_{i=1}^{n} \\theta_i^2$) ke fungsi biaya. Ini memaksa bobot model sekecil mungkin.\n",
    "\n",
    "### b. Lasso Regression\n",
    "\n",
    "Menambahkan istilah regularisasi $\\ell_1$ ($\\alpha \\sum_{i=1}^{n} |\\theta_i|$). Cenderung menghilangkan bobot fitur yang paling tidak penting (melakukan seleksi fitur otomatis).\n",
    "\n",
    "### c. Elastic Net\n",
    "\n",
    "Gabungan antara Ridge dan Lasso, dikendalikan oleh rasio campuran $r$.\n",
    "\n",
    "### d. Early Stopping\n",
    "\n",
    "Cara regularisasi dengan menghentikan pelatihan (Gradient Descent) segera setelah error validasi mencapai minimum.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Misalkan X_poly dan y dari contoh sebelumnya sudah ada\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X_poly, y)\n",
    "print(\"Ridge Intercept & Coef:\", ridge_reg.intercept_, ridge_reg.coef_)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X_poly, y)\n",
    "print(\"Lasso Intercept & Coef:\", lasso_reg.intercept_, lasso_reg.coef_)\n",
    "\n",
    "# Elastic Net\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X_poly, y)\n",
    "print(\"ElasticNet Intercept & Coef:\", elastic_net.intercept_, elastic_net.coef_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Regresi Logistik (Logistic Regression)\n",
    "\n",
    "Digunakan untuk memperkirakan probabilitas suatu instance termasuk dalam kelas tertentu (klasifikasi biner). Model ini menerapkan fungsi logistik (sigmoid) pada hasil perhitungan bobot fitur.\n",
    "Fungsi biaya yang digunakan adalah log loss (cross-entropy).\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Memuat dataset Iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, 3:]  # fitur petal width\n",
    "y = (iris[\"target\"] == 2).astype(int)  # 1 jika Iris-Virginica, else 0\n",
    "\n",
    "# Melatih model Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "# Membuat prediksi probabilitas\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "# Plot hasil\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Bukan Iris-Virginica\")\n",
    "plt.xlabel(\"Petal width (cm)\")\n",
    "plt.ylabel(\"Probabilitas\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Prediksi kelas\n",
    "print(\"\\nPrediksi untuk petal width 1.7cm:\", log_reg.predict([[1.7]]))\n",
    "print(\"Prediksi untuk petal width 1.5cm:\", log_reg.predict([[1.5]]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. . Regresi Softmax (Softmax Regression)\n",
    "\n",
    "Generalisasi dari Regresi Logistik untuk mendukung klasifikasi multikelas secara langsung. Model menghitung skor untuk setiap kelas, lalu menerapkan fungsi softmax untuk mendapatkan probabilitas. Fungsi biaya yang digunakan juga cross-entropy.\n",
    "\n",
    "```python\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X, y)\n",
    "\n",
    "print(\"\\nPrediksi Softmax untuk (petal length=5, petal width=2):\", softmax_reg.predict([[5, 2]]))\n",
    "print(\"Probabilitasnya:\", softmax_reg.predict_proba([[5, 2]]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Kesimpulan Bab\n",
    "\n",
    "Bab ini berhasil menguraikan dasar-dasar model linier, termasuk Regresi Linier dan Regresi Logistik, serta bagaimana melatihnya. Pentingnya feature scaling, learning rate, dan regularisasi dibahas secara mendalam untuk mengatasi underfitting dan overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
