{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5cf0115",
   "metadata": {},
   "source": [
    "# Bab 18: Reinforcement Learning (Pembelajaran Penguatan)\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab 18 memperkenalkan **Reinforcement Learning (RL)**, sebuah paradigma *Machine Learning* yang berbeda dari *supervised* dan *unsupervised learning*. Dalam RL, sebuah **agen (agent)** belajar bagaimana berperilaku dalam suatu **lingkungan (environment)** dengan melakukan **tindakan (actions)** dan menerima **hadiah (rewards)** atau **hukuman (penalties)**. Tujuannya adalah untuk memaksimalkan total hadiah kumulatif dari waktu ke waktu.\n",
    "\n",
    "RL telah mencapai keberhasilan luar biasa, terutama dalam permainan (misalnya, AlphaGo) dan robotika. Bab ini akan membahas konsep inti RL, beberapa algoritma utamanya, dan bagaimana mengimplementasikannya.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Konsep Inti Reinforcement Learning\n",
    "\n",
    "* **Kebijakan (Policy):** Strategi yang digunakan agen untuk memilih tindakan berdasarkan keadaan saat ini. Ini bisa bersifat deterministik atau stokastik.\n",
    "* **Fungsi Nilai (Value Function):** Memprediksi seberapa baik (dalam hal hadiah masa depan yang diharapkan) berada dalam keadaan tertentu atau melakukan tindakan tertentu dalam keadaan tertentu.\n",
    "* **Model Lingkungan:** Beberapa agen mungkin mencoba mempelajari model lingkungan itu sendiri (*model-based RL*), yang memungkinkan mereka untuk merencanakan ke depan.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Open AI Gym: Lingkungan untuk Eksperimen RL\n",
    "\n",
    "**OpenAI Gym** (sekarang dikelola oleh Farama Foundation dengan nama Gymnasium) adalah *toolkit* yang menyediakan berbagai lingkungan simulasi untuk mengembangkan dan menguji algoritma RL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf02b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gym\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Membuat lingkungan CartPole\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "# Menjalankan satu episode dengan kebijakan acak\n",
    "# for _ in range(50):\n",
    "#    action = env.action_space.sample()\n",
    "#    obs, reward, done, truncated, info = env.step(action)\n",
    "#    if done or truncated:\n",
    "#        break\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ec087",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Algoritma Policy Gradients (PG)\n",
    "Policy Gradients adalah keluarga algoritma RL di mana jaringan saraf dilatih untuk secara langsung memprediksi probabilitas setiap tindakan yang mungkin (policy-based). Jaringan ini disebut policy network.\n",
    "\n",
    "Proses Pelatihan:\n",
    "1. **Jalankan Episode**: Jalankan model (agen) di lingkungan untuk beberapa episode dan kumpulkan semua riwayat (keadaan, tindakan, hadiah).\n",
    "2. **Hitung Hadiah yang Didiskon**: Untuk setiap langkah, hitung total hadiah masa depan yang didiskon (discounted future rewards). Tindakan yang mengarah ke hadiah tinggi di masa depan akan dianggap \"baik\".\n",
    "3. **Latih Jaringan**: Latih policy network dengan gradient ascent. Gradien dihitung untuk meningkatkan probabilitas tindakan \"baik\" dan mengurangi probabilitas tindakan \"buruk\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c031ee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khalif Prawira\\AppData\\Local\\Programs\\Orange\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Membangun Policy Network\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "model_pg = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Fungsi untuk memainkan satu langkah dan menghitung gradien\n",
    "# (Ini adalah bagian dari training loop yang lebih besar)\n",
    "def play_one_step_pg(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probas = model(obs[np.newaxis])\n",
    "        # ... (logika untuk memilih aksi dan menghitung loss)\n",
    "        # loss = tf.reduce_mean(loss_fn(target_probas, probas))\n",
    "    \n",
    "    # grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # return obs, reward, done, grads\n",
    "\n",
    "# Kerangka Training Loop untuk Policy Gradients\n",
    "# for episode in range(n_training_episodes):\n",
    "#     # ... (jalankan episode, kumpulkan gradien)\n",
    "#     # ... (hitung discounted rewards)\n",
    "#     # ... (normalisasi rewards dan terapkan gradien)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1fe16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Algoritma Markov Decision Processes (MDPs) dan Q-Learning\n",
    "Banyak masalah RL dapat dimodelkan sebagai Markov Decision Processes (MDPs). MDP memiliki state dengan probabilitas transisi tetap.\n",
    "\n",
    "**Q-Learning** adalah algoritma RL populer yang mencoba mempelajari action-value (disebut Q-Value) untuk setiap pasangan state-action. Q-Value merepresentasikan hadiah masa depan yang diharapkan jika agen memulai dari state S, melakukan action A, dan kemudian mengikuti kebijakan optimal sesudahnya.\n",
    "\n",
    "Algoritma ini menggunakan Bellman equation untuk memperbarui Q-Value secara iteratif.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Deep Q-Learning (DQN)\n",
    "Untuk lingkungan dengan ruang state yang sangat besar (seperti gambar dari layar game), tidak mungkin untuk melacak Q-Value untuk setiap pasangan state-action. Deep Q-Network (DQN) menyelesaikan ini dengan menggunakan jaringan saraf untuk mengestimasi Q-Value.\n",
    "\n",
    "Fitur Utama DQN:\n",
    "* **Experience Replay**: Agen menyimpan pengalamannya (transisi state, tindakan, hadiah) dalam sebuah replay buffer. Selama pelatihan, ia mengambil sampel batch acak dari buffer ini. Ini mengurangi korelasi antar sampel dan meningkatkan efisiensi data.\n",
    "* **Target Network**: DQN menggunakan dua jaringan: online network yang dilatih di setiap langkah, dan target network yang bobotnya disalin dari online network secara berkala. Target network digunakan untuk menghitung target Q-Value, yang memberikan stabilitas pada proses pelatihan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9862a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kerangka untuk Deep Q-Network (DQN) telah disiapkan.\n"
     ]
    }
   ],
   "source": [
    "# Kerangka untuk Deep Q-Network (DQN)\n",
    "# (Asumsikan `env` dan `n_outputs` sudah didefinisikan)\n",
    "\n",
    "# 1. Replay Buffer\n",
    "# replay_buffer = collections.deque(maxlen=replay_buffer_size)\n",
    "\n",
    "# 2. Model DQN\n",
    "# model_dqn = keras.models.Sequential([\n",
    "#    keras.layers.Dense(32, activation=\"elu\", input_shape=env.observation_space.shape),\n",
    "#    keras.layers.Dense(32, activation=\"elu\"),\n",
    "#    keras.layers.Dense(n_outputs)\n",
    "# ])\n",
    "\n",
    "# 3. Fungsi untuk memilih aksi (Epsilon-Greedy Policy)\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        # Q_values = model_dqn.predict(state[np.newaxis])\n",
    "        # return np.argmax(Q_values[0])\n",
    "        pass # Placeholder\n",
    "\n",
    "# 4. Kerangka Training Step\n",
    "# def training_step(batch_size):\n",
    "#     # 1. Sampel pengalaman dari replay buffer\n",
    "#     # experiences = sample_experiences(batch_size)\n",
    "#     # 2. Hitung target Q-Values menggunakan Bellman equation\n",
    "#     #    target_Q_values = rewards + discount_factor * np.max(next_Q_values, axis=1)\n",
    "#     # 3. Latih model DQN dengan Gradient Descent pada (target_Q_values - predicted_Q_values)\n",
    "print(\"\\nKerangka untuk Deep Q-Network (DQN) telah disiapkan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0ae55",
   "metadata": {},
   "source": [
    "Training RL seringkali tidak stabil dan sangat sensitif terhadap hyperparameter. Menggunakan library yang sudah matang seperti **TF-Agents** seringkali menjadi pilihan yang lebih praktis untuk aplikasi dunia nyata."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
