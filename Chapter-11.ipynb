{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ed6f67",
   "metadata": {},
   "source": [
    "# Bab 11: Training Deep Neural Networks\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab 11 membahas berbagai tantangan yang muncul saat melatih *Deep Neural Networks* (DNN) dan menyajikan serangkaian teknik canggih untuk mengatasinya. Meskipun secara teori DNN sangat kuat, dalam praktiknya kita bisa menghadapi berbagai masalah seperti:\n",
    "* **Vanishing/Exploding Gradients:** Membuat lapisan-lapisan bawah sulit untuk dilatih.\n",
    "* **Kurangnya data latih** atau biaya pelabelan yang mahal.\n",
    "* **Pelatihan yang sangat lambat.**\n",
    "* **Overfitting** karena model memiliki jutaan parameter.\n",
    "\n",
    "Bab ini akan membahas solusi untuk setiap masalah ini, termasuk inisialisasi bobot yang lebih baik, fungsi aktivasi yang lebih canggih, *Batch Normalization*, penggunaan ulang *layer* dari model yang sudah ada (*transfer learning*), *optimizer* yang lebih cepat, dan teknik regularisasi seperti *dropout*.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Masalah Vanishing/Exploding Gradients\n",
    "\n",
    "Selama proses *backpropagation*, gradien seringkali menjadi semakin kecil saat algoritma bergerak ke lapisan yang lebih bawah. Akibatnya, bobot koneksi dari lapisan bawah hampir tidak berubah, dan pelatihan tidak pernah konvergen. Ini disebut masalah **vanishing gradients**. Kebalikannya, **exploding gradients**, terjadi ketika gradien menjadi semakin besar, menyebabkan pembaruan bobot yang sangat besar dan membuat model divergen.\n",
    "\n",
    "#### a. Inisialisasi Glorot dan He\n",
    "Untuk mengatasi masalah ini, diperlukan inisialisasi bobot acak yang cermat. Strategi seperti **inisialisasi Glorot (atau Xavier)** dan **inisialisasi He** memastikan bahwa varians dari output setiap lapisan sama dengan varians inputnya, yang secara signifikan membantu mencegah gradien menghilang atau meledak.\n",
    "\n",
    "Secara *default*, Keras menggunakan inisialisasi Glorot dengan distribusi uniform. Saat menggunakan fungsi aktivasi ReLU atau variannya, lebih baik menggunakan inisialisasi He.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604c0292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense, built=False>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Contoh penggunaan inisialisasi He\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90059ac1",
   "metadata": {},
   "source": [
    "### b. Fungsi Aktivasi Nonsaturating\n",
    "Fungsi aktivasi seperti sigmoid dan tanh memiliki area \"saturasi\" di mana turunannya mendekati nol. Ketika ini terjadi, gradien berhenti mengalir selama backpropagation. Untuk mengatasinya, digunakan fungsi aktivasi yang tidak mengalami saturasi seperti ReLU dan variannya.\n",
    "* **Leaky ReLU**: $ \\text{LeakyReLU}_{\\alpha}(z) = \\max(\\alpha z, z) $. Memperbaiki masalah \"dying ReLUs\" dengan mengizinkan sedikit gradien mengalir bahkan untuk input negatif.\n",
    "* **ELU (Exponential Linear Unit)**: Mengungguli varian ReLU lainnya, tetapi sedikit lebih lambat untuk dihitung.\n",
    "* **SELU (Scaled ELU)**: Jika arsitektur hanya terdiri dari lapisan-lapisan Dense yang berurutan, SELU dapat membuat jaringan self-normalize, yang seringkali memberikan performa superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a44f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khalif Prawira\\AppData\\Local\\Programs\\Orange\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Contoh penggunaan Leaky ReLU\n",
    "model = keras.models.Sequential([\n",
    "    # [...]\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    # [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7b7ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Batch Normalization (BN)\n",
    "BN adalah teknik yang mengatasi masalah vanishing/exploding gradients dengan cara menambahkan operasi normalisasi sebelum atau sesudah fungsi aktivasi di setiap hidden layer. Teknik ini menormalkan output dari lapisan sebelumnya dengan mengurangi rata-rata dan membaginya dengan standar deviasi, lalu menskalakan dan menggesernya.\n",
    "\n",
    "Manfaat BN:\n",
    "* Model menjadi jauh lebih tidak sensitif terhadap inisialisasi bobot.\n",
    "* Memungkinkan penggunaan learning rate yang lebih besar, yang mempercepat pelatihan.\n",
    "* Bertindak sebagai regularizer, mengurangi kebutuhan teknik regularisasi lain seperti dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0401c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khalif Prawira\\AppData\\Local\\Programs\\Orange\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Implementasi Batch Normalization di Keras\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7491ebd",
   "metadata": {},
   "source": [
    "Penempatan layer BN biasanya setelah layer Dense (sebelum aktivasi) atau setelah aktivasi. Keduanya umum digunakan dan hasilnya cenderung serupa.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Gradient Clipping\n",
    "Teknik ini sangat berguna untuk mencegah exploding gradients, terutama dalam Recurrent Neural Networks (RNN). Caranya adalah dengan membatasi nilai gradien agar tidak melebihi threshold tertentu selama backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731e5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menerapkan Gradient Clipping di Keras\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0) # atau clipnorm=1.0\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f00c70",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Transfer Learning dengan Keras\n",
    "Daripada melatih DNN dari awal, seringkali jauh lebih cepat dan efektif untuk menggunakan kembali lapisan-lapisan dari jaringan yang sudah dilatih pada dataset besar yang serupa (misalnya, ImageNet). Teknik ini disebut Transfer Learning.\n",
    "\n",
    "Prosesnya:\n",
    "\n",
    "1. Bekukan lapisan-lapisan awal (Freeze Layers): Lapisan-lapisan awal dari model pre-trained biasanya mendeteksi fitur-fitur tingkat rendah (garis, tepi, bentuk). Bobotnya diatur agar tidak dapat dilatih (layer.trainable = False).\n",
    "2. Ganti atau Tambah Lapisan Output: Ganti lapisan output asli dengan lapisan baru yang sesuai dengan tugas Anda.\n",
    "3. Latih Model: Latih model pada dataset baru Anda. Awalnya, hanya lapisan baru yang dilatih.\n",
    "4. (Opsional) Fine-Tuning: Setelah beberapa epoch, Anda bisa \"mencairkan\" (unfreeze) beberapa lapisan atas dari model pre-trained dan melatihnya dengan learning rate yang sangat kecil untuk sedikit menyesuaikannya dengan data baru Anda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e97998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khalif Prawira\\AppData\\Local\\Programs\\Orange\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Contoh Transfer Learning menggunakan Xception pada Fashion MNIST\n",
    "# (Catatan: Fashion MNIST adalah gambar grayscale, contoh ini disederhanakan)\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Memuat model Xception yang sudah dilatih di ImageNet, tanpa lapisan atasnya\n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                   include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(avg)\n",
    "model_transfer = keras.Model(inputs=base_model.input, outputs=[output])\n",
    "\n",
    "# Membekukan bobot dari base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile dan latih model dengan lapisan baru\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
    "model_transfer.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "# history = model_transfer.fit(X_train, y_train, ...) # Perlu preprocessing data agar sesuai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbcc3fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Faster Optimizers\n",
    "Mengganti optimizer SGD standar dengan yang lebih canggih dapat secara signifikan mempercepat pelatihan. Beberapa optimizer populer:\n",
    "\n",
    "* **Momentum optimization**: Mempercepat konvergensi dengan menambahkan \"momentum\" pada pembaruan bobot.\n",
    "* **Nesterov Accelerated Gradient (NAG)**: Varian dari momentum yang sedikit lebih cepat.\n",
    "* **AdaGrad, RMSProp, Adam, dan Nadam**: Optimizer adaptif yang secara otomatis menyesuaikan learning rate untuk setiap parameter, seringkali menjadi pilihan default yang sangat baik. Adam dan Nadam adalah yang paling sering direkomendasikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5d2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan optimizer Adam di Keras\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5aa4d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Learning Rate Scheduling\n",
    "Menyesuaikan learning rate selama pelatihan dapat membantu model konvergen lebih cepat dan mencapai solusi yang lebih baik. Beberapa strategi penjadwalan (scheduling) umum:\n",
    "\n",
    "* **Power scheduling**: Mengurangi LR secara bertahap di setiap iterasi.\n",
    "* **Exponential scheduling**: Mengurangi LR sebesar faktor 10 setiap S langkah.\n",
    "* **Piecewise constant scheduling**: Menggunakan learning rate konstan untuk beberapa epoch, lalu menguranginya.\n",
    "* **Performance scheduling**: Mengukur validation error dan mengurangi LR ketika error berhenti menurun.\n",
    "* **1cycle scheduling**: Menaikkan LR dari nilai rendah ke tinggi, lalu menurunkannya kembali selama satu siklus pelatihan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492c7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh implementasi Performance Scheduling\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "# history = model.fit(X_train, y_train, ..., callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791d6e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8. Regularisasi untuk Mencegah Overfitting\n",
    "* **$\\ell_1$ and $\\ell_2$ Regularization**: Menambahkan penalti pada loss function berdasarkan nilai absolut ($\\ell_1$) atau kuadrat ($\\ell_2$) dari bobot model.\n",
    "* **Dropout**: Pada setiap langkah pelatihan, setiap neuron (termasuk input, tetapi tidak output) memiliki probabilitas untuk \"dijatuhkan\" (dropped out) untuk sementara. Ini memaksa neuron lain untuk belajar fitur yang lebih robust.\n",
    "* **Max-Norm Regularization**: Membatasi bobot koneksi masuk agar tidak melebihi nilai maksimum tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ea69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi Dropout di Keras\n",
    "model_dropout = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
