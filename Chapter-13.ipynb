{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ce9ea5",
   "metadata": {},
   "source": [
    "# Bab 13: Loading and Preprocessing Data with TensorFlow\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab 13 membahas salah satu aspek paling fundamental dan seringkali paling memakan waktu dalam proyek *Machine Learning*: memuat dan memproses data. Proses ini bisa menjadi sangat rumit, terutama ketika berhadapan dengan dataset yang sangat besar yang tidak muat dalam memori, atau ketika memerlukan langkah pra-pemrosesan yang kompleks.\n",
    "\n",
    "Bab ini memperkenalkan **Data API (`tf.data`)** dari TensorFlow, sebuah *toolset* yang kuat dan efisien untuk membuat *pipeline* input data. Dengan Data API, Anda dapat dengan mudah membaca data dari berbagai sumber, melakukan transformasi yang kompleks, dan mengalirkannya ke model Anda secara efisien.\n",
    "\n",
    "Topik utama yang dibahas:\n",
    "* **Data API (`tf.data`):** Cara membuat *pipeline* data yang efisien, termasuk *chaining transformations*, *shuffling*, *prefetching*, dan paralelisasi.\n",
    "* **Format TFRecord:** Format biner portabel dari TensorFlow yang sangat efisien untuk menyimpan dan mengakses data dalam jumlah besar.\n",
    "* **Preprocessing Data:** Cara membuat *preprocessing layer* menggunakan Keras atau `tf.io` untuk menangani fitur numerik dan kategorikal.\n",
    "* **Proyek TF-Transform dan TFDS:** Pengenalan singkat tentang *tool* tingkat tinggi untuk manajemen data dan *preprocessing*.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data API (`tf.data`)\n",
    "\n",
    "Data API berpusat pada konsep **dataset**, yang merepresentasikan urutan *item*. Biasanya, setiap *item* adalah satu *instance* (pasangan fitur dan label), tetapi bisa juga berupa apa saja. TensorFlow menyediakan berbagai cara untuk membuat dataset.\n",
    "\n",
    "#### a. Membuat Pipeline Data\n",
    "Cara paling sederhana untuk membuat dataset adalah dengan `tf.data.Dataset.from_tensor_slices()`. Fungsi ini mengambil sebuah tensor dan membuat dataset yang *item*-nya adalah semua *slice* dari tensor tersebut.\n",
    "\n",
    "Setelah dataset dibuat, Anda dapat menerapkan serangkaian transformasi dengan menyambungkannya (*chaining*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b071429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Membuat dataset dari tensor\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Chaining transformations\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "\n",
    "# Iterasi melalui dataset\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b340357",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "`tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)`\n",
    "`tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)`\n",
    "\n",
    "### b. Transformasi Umum\n",
    "map(): Menerapkan fungsi kustom pada setiap item.\n",
    "filter(): Menyaring item berdasarkan predikat.\n",
    "shuffle(): Mengacak item dalam dataset. Penting untuk performa Gradient Descent.\n",
    "batch(): Mengelompokkan item ke dalam batch.\n",
    "repeat(): Mengulang dataset beberapa kali.\n",
    "prefetch(): Mempersiapkan batch data berikutnya saat model sedang berlatih dengan batch saat ini, yang dapat secara signifikan meningkatkan performa.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Format File TFRecord\n",
    "Untuk dataset yang sangat besar, seringkali tidak efisien untuk memiliki ribuan atau jutaan file kecil. Format TFRecord adalah solusi TensorFlow untuk ini. Ini adalah format biner sederhana yang memungkinkan Anda menyimpan urutan record biner. Ini sangat efisien untuk dibaca dan sangat cocok untuk data yang besar.\n",
    "\n",
    "Setiap record di dalam file TFRecord adalah string byte, dan untuk membuatnya portabel, kita perlu melakukan serialisasi data terlebih dahulu. Format serialisasi standar yang digunakan adalah **protocol buffers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3050374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh menulis dan membaca file TFRecord\n",
    "# (Ini adalah contoh konseptual, implementasi penuh memerlukan lebih banyak detail)\n",
    "\n",
    "# Menulis ke TFRecord\n",
    "# with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "#     f.write(b\"Contoh record pertama\")\n",
    "#     f.write(b\"Contoh record kedua\")\n",
    "\n",
    "# Membaca dari TFRecord dengan tf.data API\n",
    "# filepath = [\"my_data.tfrecord\"]\n",
    "# dataset = tf.data.TFRecordDataset(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44905a18",
   "metadata": {},
   "source": [
    "### a. Protocol Buffers dan tf.train.Example\n",
    "`tf.train.Example` adalah protocol buffer yang fleksibel untuk merepresentasikan satu instance data. Ia berisi daftar fitur, di mana setiap fitur bisa berupa ByteList, FloatList, atau Int64List.\n",
    "\n",
    "### b. Memuat dan Parsing TFRecord\n",
    "Untuk memuat data dari file TFRecord, kita menggunakan `tf.data.TFRecordDataset`. Setelah dimuat, setiap record (yang masih dalam bentuk string byte terserialisasi) perlu di-parsing menggunakan `tf.io.parse_single_example()`.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Preprocessing Fitur Input\n",
    "Sebelum memasukkan data ke model, fitur seringkali perlu di-preprocess.\n",
    "\n",
    "### a. Preprocessing dengan Keras Preprocessing Layers\n",
    "Keras menyediakan serangkaian layer untuk preprocessing yang dapat dimasukkan langsung ke dalam model Anda. Ini sangat praktis karena layer ini akan secara otomatis diterapkan pada data saat pelatihan dan inferensi.\n",
    "\n",
    "* **Normalisasi**: `keras.layers.Normalization` dapat menormalkan fitur numerik. Layer ini akan menghitung rata-rata dan standar deviasi dari data pelatihan saat metode `.adapt()` dipanggil.\n",
    "* **Diskretisasi** (Binning): `keras.layers.Discretization` mengubah fitur numerik berkelanjutan menjadi fitur kategorikal dengan membaginya ke dalam beberapa bin.\n",
    "* **Fitur Kategorikal**:\n",
    "    * `keras.layers.StringLookup` atau `IntegerLookup`: Mengubah fitur kategorikal (string atau integer) menjadi indeks integer.\n",
    "    * `keras.layers.Embedding`: Mengubah indeks integer menjadi embedding vector padat.\n",
    "    * `keras.layers.CategoryEncoding`: Melakukan one-hot encoding pada fitur kategorikal.\n",
    "* **Preprocessing Gambar**: Keras juga menyediakan layer seperti Resizing, Rescaling, RandomFlip, dan RandomRotation untuk augmentasi data gambar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25abda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh pipeline preprocessing untuk data numerik dan kategorikal\n",
    "# (Asumsikan sudah ada data latih)\n",
    "\n",
    "# Buat layer untuk normalisasi fitur numerik\n",
    "# norm_layer = keras.layers.Normalization()\n",
    "# norm_layer.adapt(X_train_num)\n",
    "\n",
    "# Buat layer untuk one-hot encoding fitur kategorikal\n",
    "# cat_layer = keras.layers.CategoryEncoding(num_tokens=num_cat_classes)\n",
    "\n",
    "# ... kemudian gabungkan dengan model menggunakan Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5bfa8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Dataset TensorFlow (TFDS)\n",
    "TensorFlow Datasets (TFDS) adalah library yang menyediakan akses mudah ke puluhan dataset machine learning yang siap pakai. Ini menangani semua proses pengunduhan, pemisahan, dan pembuatan `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c05e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasetsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tensorflow_datasets-4.9.9-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-datasets) (2.3.0)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.9-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading etils-1.12.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-datasets) (5.29.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-datasets) (6.1.0)\n",
      "Collecting pyarrow (from tensorflow-datasets)\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting simple_parsing (from tensorflow-datasets)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.17.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-datasets) (3.1.0)\n",
      "Requirement already satisfied: toml in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-datasets) (1.17.2)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.5.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.4.5)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.21.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.12.14)\n",
      "Requirement already satisfied: attrs>=18.2.0 in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from dm-tree->tensorflow-datasets) (24.3.0)\n",
      "Requirement already satisfied: six in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from promise->tensorflow-datasets) (1.17.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow-datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.70.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\khalif prawira\\appdata\\local\\programs\\orange\\lib\\site-packages (from tqdm->tensorflow-datasets) (0.4.6)\n",
      "Downloading tensorflow_datasets-4.9.9-py3-none-any.whl (5.3 MB)\n",
      "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.3 MB 465.6 kB/s eta 0:00:11\n",
      "   --- ------------------------------------ 0.5/5.3 MB 465.6 kB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 0.8/5.3 MB 550.1 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.8/5.3 MB 550.1 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 1.0/5.3 MB 592.2 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 1.0/5.3 MB 592.2 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 1.3/5.3 MB 610.0 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 1.6/5.3 MB 660.3 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.8/5.3 MB 708.8 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.1/5.3 MB 752.7 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 2.4/5.3 MB 784.9 kB/s eta 0:00:04\n",
      "   ------------------- -------------------- 2.6/5.3 MB 816.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 2.9/5.3 MB 851.7 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.1/5.3 MB 854.4 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 3.4/5.3 MB 890.6 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 3.7/5.3 MB 920.3 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 3.9/5.3 MB 947.1 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.2/5.3 MB 967.8 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.5/5.3 MB 994.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.7/5.3 MB 997.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  5.2/5.3 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.3/5.3 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading etils-1.12.2-py3-none-any.whl (167 kB)\n",
      "Downloading dm_tree-0.1.9-cp311-cp311-win_amd64.whl (101 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-20.0.0-cp311-cp311-win_amd64.whl (25.8 MB)\n",
      "   ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.8 MB 1.3 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.8/25.8 MB 1.3 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 1.0/25.8 MB 1.4 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 1.3/25.8 MB 1.3 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 1.6/25.8 MB 1.3 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 1.8/25.8 MB 1.4 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 2.1/25.8 MB 1.3 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 2.4/25.8 MB 1.3 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 2.6/25.8 MB 1.3 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 2.9/25.8 MB 1.3 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 3.1/25.8 MB 1.3 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.4/25.8 MB 1.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 3.7/25.8 MB 1.3 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 3.9/25.8 MB 1.3 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 4.2/25.8 MB 1.3 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 4.7/25.8 MB 1.3 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 5.0/25.8 MB 1.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 5.2/25.8 MB 1.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 5.2/25.8 MB 1.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 5.5/25.8 MB 1.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 5.8/25.8 MB 1.3 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 6.0/25.8 MB 1.3 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 6.3/25.8 MB 1.3 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 6.8/25.8 MB 1.3 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 7.1/25.8 MB 1.3 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 7.1/25.8 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 7.6/25.8 MB 1.3 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 7.9/25.8 MB 1.3 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 8.1/25.8 MB 1.3 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 8.4/25.8 MB 1.3 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 8.7/25.8 MB 1.3 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 8.9/25.8 MB 1.3 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 9.2/25.8 MB 1.3 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 9.7/25.8 MB 1.3 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 10.0/25.8 MB 1.3 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 10.2/25.8 MB 1.3 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 10.5/25.8 MB 1.3 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 10.7/25.8 MB 1.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 11.3/25.8 MB 1.3 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 11.5/25.8 MB 1.4 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 11.8/25.8 MB 1.4 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 12.1/25.8 MB 1.4 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 12.6/25.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 12.8/25.8 MB 1.4 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 13.4/25.8 MB 1.4 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 13.6/25.8 MB 1.4 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 14.2/25.8 MB 1.4 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 14.4/25.8 MB 1.4 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 14.9/25.8 MB 1.4 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 15.2/25.8 MB 1.4 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 15.7/25.8 MB 1.5 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 16.0/25.8 MB 1.5 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 16.5/25.8 MB 1.5 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 17.0/25.8 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 17.3/25.8 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 17.6/25.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 18.1/25.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 18.6/25.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 18.9/25.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 19.4/25.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 19.9/25.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 20.2/25.8 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.7/25.8 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 21.2/25.8 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 21.5/25.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 22.0/25.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 22.5/25.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 23.1/25.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.6/25.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 24.1/25.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 24.4/25.8 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.9/25.8 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.8 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.8/25.8 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.17.1-py3-none-any.whl (31 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21544 sha256=e5fee6bbf4920c67b15fa9c5d95aaf62d0566b808ceb4bd50f10e57d919ff33c\n",
      "  Stored in directory: c:\\users\\khalif prawira\\appdata\\local\\pip\\cache\\wheels\\90\\74\\b1\\9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: pyarrow, promise, immutabledict, etils, einops, docstring-parser, dm-tree, tensorflow-metadata, simple_parsing, tensorflow-datasets\n",
      "Successfully installed dm-tree-0.1.9 docstring-parser-0.16 einops-0.8.1 etils-1.12.2 immutabledict-4.2.1 promise-2.3 pyarrow-20.0.0 simple_parsing-0.1.7 tensorflow-datasets-4.9.9 tensorflow-metadata-1.17.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder C:\\Users\\Khalif Prawira\\tensorflow_datasets\\mnist\\3.0.1 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Khalif Prawira\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khalif Prawira\\AppData\\Local\\Programs\\Orange\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.00 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.91 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.74 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.61 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.55 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.49 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.61s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.92s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:04<00:01,  1.34s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:04<00:01,  1.34s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:04<00:01,  1.34s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:04<00:01,  1.34s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:05<00:01,  1.34s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.34s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.34s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:07<00:01,  1.34s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:08<00:01,  1.34s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:08<00:01,  1.34s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:09<00:01,  1.34s/ url]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:09<00:00,  2.70s/ url]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:09<00:00,  2.70s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:09<00:00,  2.70s/ url]\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:10<00:00,  2.50s/ file]\n",
      "Dl Size...: 100%|██████████| 10/10 [00:10<00:00,  1.00s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:10<00:00,  2.50s/ url]\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\Khalif Prawira\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-datasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Memuat dataset (misalnya, MNIST)\n",
    "# as_supervised=True akan mengembalikan tuple (input, label)\n",
    "dataset, info = tfds.load(\"mnist\", split=\"train\", as_supervised=True, with_info=True)\n",
    "\n",
    "# Dataset yang dikembalikan sudah berupa objek tf.data.Dataset\n",
    "# dataset = dataset.map(preprocess_function).shuffle(buffer_size).batch(batch_size)\n",
    "# model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c5527",
   "metadata": {},
   "source": [
    "Menggunakan TFDS dapat secara signifikan menyederhanakan dan mempercepat alur kerja Anda, memungkinkan Anda untuk fokus pada pembangunan dan pelatihan model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
