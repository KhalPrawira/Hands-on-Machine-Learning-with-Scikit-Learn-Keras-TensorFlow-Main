{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1741227",
   "metadata": {},
   "source": [
    "# Bab 15: Processing Sequences Using RNNs, CNNs, and Transformers\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab 15 membahas Jaringan Saraf Tiruan (JST) yang dirancang khusus untuk memproses data sekuensial atau urutan, seperti teks, suara, video, atau deret waktu. **Recurrent Neural Networks (RNNs)** adalah jenis JST yang memiliki \"memori\" internal yang memungkinkan mereka mempertahankan informasi dari langkah waktu sebelumnya, menjadikannya sangat cocok untuk tugas-tugas yang melibatkan dependensi temporal.\n",
    "\n",
    "Bab ini akan membahas:\n",
    "* Arsitektur RNN dasar dan tantangan pelatihannya.\n",
    "* Varian RNN yang lebih canggih seperti **Long Short-Term Memory (LSTM)** dan **Gated Recurrent Unit (GRU)** yang mengatasi masalah memori jangka pendek.\n",
    "* Penggunaan RNN untuk tugas peramalan deret waktu (*time series forecasting*).\n",
    "* Arsitektur **WaveNet** yang menggunakan lapisan konvolusional 1D untuk memproses urutan secara efisien.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Berbeda dengan jaringan *feedforward* standar, RNN memiliki koneksi berulang (*recurrent connections*) yang memungkinkan informasi untuk bertahan. Sebuah sel RNN menerima input dari langkah waktu saat ini dan juga output dari langkah waktu sebelumnya. *State* tersembunyi ($h_t$) ini bertindak sebagai bentuk memori.\n",
    "\n",
    "#### a. Input dan Output Sequences\n",
    "RNN dapat menangani berbagai jenis tugas berdasarkan input dan outputnya:\n",
    "* **Sequence-to-sequence:** Menerjemahkan kalimat.\n",
    "* **Sequence-to-vector:** Analisis sentimen (input kalimat, output kelas sentimen).\n",
    "* **Vector-to-sequence:** Membuat *caption* untuk gambar.\n",
    "\n",
    "#### b. Masalah Pelatihan RNN\n",
    "RNN seringkali sulit dilatih karena masalah *vanishing/exploding gradients* (seperti pada DNN) yang terjadi sepanjang dimensi waktu. Ketika urutan data sangat panjang, gradien bisa menghilang, membuat model kesulitan belajar dependensi jangka panjang.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Peramalan Deret Waktu (Time Series Forecasting)\n",
    "\n",
    "RNN sangat cocok untuk peramalan deret waktu, seperti memprediksi harga saham atau cuaca.\n",
    "\n",
    "#### a. Mempersiapkan Data\n",
    "Untuk melatih RNN pada data deret waktu, kita perlu mengubahnya menjadi jendela data (*windowed dataset*). Setiap *instance* terdiri dari jendela input (misalnya, 20 langkah waktu sebelumnya) dan jendela target (misalnya, 1 langkah waktu berikutnya atau beberapa langkah ke depan).\n",
    "\n",
    "#### b. Baseline Metrics\n",
    "Sebelum membangun model yang kompleks, penting untuk memiliki metrik dasar. Pendekatan naif seperti memprediksi bahwa nilai berikutnya akan sama dengan nilai saat ini seringkali menjadi *baseline* yang baik.\n",
    "\n",
    "#### c. Implementasi Simple RNN dengan Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khalif Prawira\\AppData\\Local\\Programs\\Orange\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fungsi untuk menghasilkan data deret waktu sintetis\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    time = time[np.newaxis, :, np.newaxis]  # shape (1, n_steps, 1)\n",
    "    offsets1 = offsets1[:, np.newaxis, :]   # shape (batch_size, 1, 1)\n",
    "    offsets2 = offsets2[:, np.newaxis, :]   # shape (batch_size, 1, 1)\n",
    "    freq1 = freq1[:, np.newaxis, :]         # shape (batch_size, 1, 1)\n",
    "    freq2 = freq2[:, np.newaxis, :]         # shape (batch_size, 1, 1)\n",
    "    wave1 = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
    "    wave2 = 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))\n",
    "    noise = 0.1 * (np.random.rand(batch_size, n_steps, 1) - 0.5)\n",
    "    series = wave1 + wave2 + noise\n",
    "    return series.astype(np.float32)\n",
    "\n",
    "# Membuat dataset\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
    "\n",
    "# Model RNN sederhana\n",
    "model_rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "model_rnn.compile(loss=\"mse\", optimizer=optimizer)\n",
    "# history = model_rnn.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad75b9",
   "metadata": {},
   "source": [
    "### d. Deep RNNs\n",
    "Menumpuk beberapa lapisan RNN dapat menghasilkan performa yang lebih baik. Penting untuk mengatur `return_sequences=True` untuk semua lapisan RNN kecuali lapisan terakhir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb237603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Deep RNN\n",
    "model_deep_rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1) # Lapisan terakhir hanya mengembalikan output akhir\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e7714",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Menangani Dependensi Jangka Panjang\n",
    "Masalah utama RNN sederhana adalah mereka memiliki memori jangka pendek karena masalah vanishing gradients. Untuk mengatasi ini, digunakan sel yang lebih canggih.\n",
    "\n",
    "### a. Sel LSTM (Long Short-Term Memory)\n",
    "Sel LSTM adalah varian RNN yang sangat sukses. Ia memiliki mekanisme gerbang (gate mechanism) yang secara eksplisit mengontrol informasi mana yang harus diingat dan dilupakan.\n",
    "\n",
    "* Forget Gate: Mengontrol informasi mana dari long-term state yang harus dibuang.\n",
    "* Input Gate: Mengontrol informasi mana dari input saat ini yang harus disimpan di long-term state.\n",
    "* Output Gate: Mengontrol informasi mana dari long-term state yang harus dibaca dan dikeluarkan sebagai output pada langkah waktu saat ini.\n",
    "Mekanisme ini memungkinkan LSTM untuk belajar dan mengingat dependensi dalam urutan yang sangat panjang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675af3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dengan lapisan LSTM\n",
    "model_lstm = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40b57",
   "metadata": {},
   "source": [
    "### b. Sel GRU (Gated Recurrent Unit)\n",
    "Sel GRU adalah versi yang lebih sederhana dari LSTM. Ia menggabungkan cell state dan hidden state serta forget gate dan input gate menjadi satu update gate. Arsitekturnya yang lebih sederhana membuatnya sedikit lebih cepat untuk dilatih.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Arsitektur WaveNet untuk Urutan Data\n",
    "Arsitektur WaveNet, yang awalnya diusulkan oleh DeepMind untuk audio generatif, juga sangat baik untuk data deret waktu. Ia menggunakan tumpukan lapisan konvolusional 1D (Conv1D) dengan dilated convolutions.\n",
    "\n",
    "* **Causal Convolutions**: Lapisan konvolusional yang memastikan output pada langkah waktu `t` hanya bergantung pada input dari langkah waktu `t` dan sebelumnya, bukan dari masa depan.\n",
    "* **Dilated Convolutions**: Dengan melompati input pada setiap langkah (dilation rate), satu lapisan dapat memiliki receptive field yang sangat besar, memungkinkannya untuk menangkap pola jangka panjang dengan lebih efisien daripada RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model WaveNet untuk peramalan multi-langkah\n",
    "# (Memprediksi 10 langkah ke depan)\n",
    "Y = np.empty((10000, n_steps, 10))\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train, Y_valid = Y[:7000], Y[7000:9000]\n",
    "\n",
    "model_wavenet = keras.models.Sequential()\n",
    "model_wavenet.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
    "for rate in (1, 2, 4, 8) * 2: # Tumpukan blok dilatasi\n",
    "    model_wavenet.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
    "                                 activation=\"relu\", dilation_rate=rate))\n",
    "model_wavenet.add(keras.layers.Conv1D(filters=10, kernel_size=1)) # Lapisan output\n",
    "\n",
    "# Compile dan latih model\n",
    "# model_wavenet.compile(loss=\"mse\", optimizer=\"adam\", ...)\n",
    "# history = model_wavenet.fit(X_train, Y_train[..., -1], ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7202920",
   "metadata": {},
   "source": [
    "Arsitektur ini dapat jauh lebih cepat untuk dilatih daripada RNN, terutama untuk urutan data yang sangat panjang."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
