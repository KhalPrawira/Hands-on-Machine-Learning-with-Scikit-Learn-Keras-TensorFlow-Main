{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e57703",
   "metadata": {},
   "source": [
    "# Bab 5: Support Vector Machines (SVM)\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab ini berfokus pada *Support Vector Machine* (SVM), sebuah model *machine learning* yang sangat kuat dan serbaguna. SVM mampu melakukan klasifikasi linier maupun non-linier, regresi, dan bahkan deteksi *outlier*. Model ini sangat populer dan menjadi salah satu yang wajib dikuasai, terutama untuk masalah klasifikasi yang kompleks dengan dataset berukuran kecil hingga sedang.\n",
    "\n",
    "Bab ini akan membahas konsep inti SVM, mulai dari:\n",
    "* Bagaimana SVM bekerja untuk klasifikasi linier (*hard margin* dan *soft margin*).\n",
    "* Cara menangani data non-linier menggunakan *kernel trick*.\n",
    "* Penerapan SVM untuk tugas regresi.\n",
    "* Mekanisme matematika di balik SVM.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Klasifikasi SVM Linier (Linear SVM Classification)\n",
    "\n",
    "Ide fundamental di balik SVM adalah menemukan \"jalan\" (*street*) terluas di antara kelas-kelas data yang berbeda. Tujuannya adalah untuk memaksimalkan *margin*, yaitu jarak antara garis pemisah (*decision boundary*) dan *instance* pelatihan terdekat dari setiap kelas. *Instance* yang berada di tepi \"jalan\" ini disebut **support vectors**.\n",
    "\n",
    "#### a. Klasifikasi Hard Margin\n",
    "*Hard margin classification* hanya berfungsi jika data dapat dipisahkan secara linier dengan sempurna dan sangat sensitif terhadap *outlier*.\n",
    "\n",
    "#### b. Klasifikasi Soft Margin\n",
    "Untuk mengatasi kelemahan *hard margin*, digunakan model yang lebih fleksibel, yaitu *soft margin classification*. Tujuannya adalah menyeimbangkan antara menjaga \"jalan\" selebar mungkin dan membatasi pelanggaran margin (*margin violations*), yaitu *instance* yang berada di dalam \"jalan\" atau bahkan di sisi yang salah.\n",
    "\n",
    "Di Scikit-Learn, keseimbangan ini dikendalikan oleh *hyperparameter* `C`:\n",
    "* Nilai `C` yang kecil menghasilkan \"jalan\" yang lebih lebar tetapi lebih banyak pelanggaran margin.\n",
    "* Nilai `C` yang besar menghasilkan \"jalan\" yang lebih sempit tetapi lebih sedikit pelanggaran margin.\n",
    "\n",
    "Berikut adalah contoh implementasi `LinearSVC` pada dataset Iris:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "# Membuat pipeline untuk scaling dan klasifikasi SVM linier\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "# Prediksi\n",
    "print(svm_clf.predict([[5.5, 1.7]]))\n",
    "# Output: [1.]\n",
    "```\n",
    "---\n",
    "\n",
    "### 3. Klasifikasi SVM Non-Linier (Nonlinear SVM Classification)\n",
    "\n",
    "Untuk menangani dataset yang tidak dapat dipisahkan secara linier, strategi utamanya adalah dengan menambahkan fitur baru, seperti fitur polinomial. Dengan menambahkan fitur baru, dataset yang tadinya non-linier bisa menjadi dapat dipisahkan secara linier di dimensi yang lebih tinggi.\n",
    "\n",
    "### a. Fitur Polinomial\n",
    "\n",
    "Menambahkan fitur polinomial dapat dilakukan dengan PolynomialFeatures dari Scikit-Learn.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "```\n",
    "\n",
    "### b. Kernel Polinomial (Polynomial Kernel)\n",
    "\n",
    "Menambahkan fitur polinomial bisa sangat lambat jika derajatnya tinggi. Untungnya, SVM menawarkan solusi cerdas yang disebut kernel trick. Kernel trick memungkinkan kita mendapatkan hasil yang sama seolah-olah kita menambahkan banyak fitur polinomial, tanpa benar-benar harus menambahkannya. Ini sangat efisien secara komputasi.\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Menggunakan kernel polinomial derajat 3\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)\n",
    "```\n",
    "\n",
    "### c. Kernel Gaussian RBF (Gaussian RBF Kernel)\n",
    "\n",
    "Metode populer lainnya adalah dengan menambahkan fitur yang dihitung menggunakan fungsi kemiripan (similarity function). Salah satu yang paling umum adalah Gaussian Radial Basis Function (RBF).\n",
    "$$\\phi_{\\gamma}(\\mathbf{x}, \\boldsymbol{\\ell}) = \\exp(-\\gamma ||\\mathbf{x} - \\boldsymbol{\\ell}||^2)$$\n",
    "\n",
    "Kernel RBF juga dapat digunakan dengan kernel trick di Scikit-Learn. Ini adalah salah satu kernel yang paling kuat dan serbaguna.\n",
    "\n",
    "* Hyperparameter gamma ($\\gamma$): Bertindak seperti parameter regularisasi. Jika model overfitting, coba kurangi nilai gamma. Jika underfitting, coba tingkatkan.\n",
    "*  Hyperparameter C: Sama seperti pada SVM linier, mengontrol trade-off antara margin dan pelanggaran.\n",
    "\n",
    "```python\n",
    "# Menggunakan kernel Gaussian RBF\n",
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)\n",
    "```\n",
    "\n",
    "**Aturan Praktis Pemilihan Kernel:**\n",
    "\n",
    "Selalu coba LinearSVC terlebih dahulu. Jika dataset tidak terlalu besar, coba SVC(kernel=\"rbf\"). Jika membutuhkan performa lebih, Anda dapat mencoba kernel lain atau melakukan pencarian hyperparameter.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Regresi SVM (SVM Regression)\n",
    "\n",
    "\n",
    "SVM juga dapat digunakan untuk tugas regresi. Triknya adalah membalikkan tujuan: alih-alih mencoba menemukan \"jalan\" terluas di antara dua kelas, regresi SVM mencoba memasukkan sebanyak mungkin instance ke dalam \"jalan\" (street) sambil membatasi pelanggaran margin. Lebar \"jalan\" dikendalikan oleh hyperparameter epsilon ($\\epsilon$).\n",
    "\n",
    "### a. Regresi SVM Linier\n",
    "```python\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Membuat data acak\n",
    "np.random.seed(42)\n",
    "m = 50\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = (4 + 3 * X + np.random.randn(m, 1)).ravel()\n",
    "\n",
    "# Melatih model LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
    "svm_reg.fit(X, y)\n",
    "```\n",
    "\n",
    "### b. Regresi SVM Non-Linier\n",
    "\n",
    "Sama seperti klasifikasi, untuk menangani data regresi non-linier, kita bisa menggunakan model SVM dengan kernel, seperti kernel polinomial.\n",
    "```python\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Membuat data kuadratik\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()\n",
    "\n",
    "# Melatih SVR dengan kernel polinomial derajat 2\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Di Balik Layar (Under the Hood)\n",
    "\n",
    "Bagian ini membahas dasar-dasar matematika SVM, termasuk:\n",
    "\n",
    "* **Fungsi Keputusan dan Prediksi**: Bagaimana SVM membuat prediksi berdasarkan fungsi $w^\\intercal \\cdot x + b$.\n",
    "* **Tujuan Pelatihan**: Menjelaskan bagaimana tujuan pelatihan untuk hard margin dan soft margin dirumuskan secara matematis untuk meminimalkan kemiringan (slope) dan pelanggaran margin.\n",
    "* **Quadratic Programming (QP)**: Masalah optimisasi yang diselesaikan oleh SVM.\n",
    "* **The Dual Problem**: Sebuah pendekatan alternatif untuk menyelesaikan masalah QP yang memungkinkan penggunaan kernel trick.\n",
    "* **Kernel**: Penjelasan matematis tentang cara kerja kernel trick."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
