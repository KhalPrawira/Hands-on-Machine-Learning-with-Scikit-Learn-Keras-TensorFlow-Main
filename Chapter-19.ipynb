{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4810a6f9",
   "metadata": {},
   "source": [
    "# Bab 19: Melatih dan Menyebarkan Model TensorFlow dalam Skala Besar\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab 19 adalah bab yang sangat praktis, berfokus pada transisi dari melatih model *Machine Learning* di lingkungan penelitian (misalnya, Jupyter Notebook) ke menyebarkannya di lingkungan produksi. Ini mencakup berbagai alat dan praktik terbaik untuk pelatihan skala besar, *deployment*, dan pemantauan.\n",
    "\n",
    "Bab ini akan membahas:\n",
    "* **Distribusi Pelatihan:** Melatih model di banyak perangkat (GPU/TPU) atau di banyak mesin.\n",
    "* **TensorFlow Serving:** Menyebarkan model terlatih untuk inferensi.\n",
    "* **TensorFlow Lite:** Mengoptimalkan model untuk perangkat seluler dan *edge*.\n",
    "* **TensorFlow.js:** Menjalankan model di browser.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Pelatihan dalam Skala Besar (*Training at Scale*)\n",
    "\n",
    "Melatih model *Deep Learning* yang besar pada dataset yang sangat besar seringkali memerlukan lebih dari satu mesin atau GPU.\n",
    "\n",
    "#### a. Paralelisme Data vs Paralelisme Model\n",
    "* **Paralelisme Data:** Setiap perangkat (GPU/CPU/TPU) menerima salinan model yang sama dan melatihnya pada *subset* data yang berbeda secara paralel. Gradien kemudian diagregasi untuk memperbarui bobot model.\n",
    "* **Paralelisme Model:** Model dipecah menjadi beberapa bagian, dan setiap bagian dijalankan pada perangkat yang berbeda.\n",
    "\n",
    "TensorFlow menyediakan `tf.distribute.Strategy` API untuk mempermudah pelatihan terdistribusi. `MirroredStrategy` adalah contoh yang baik untuk paralelisme data pada beberapa GPU di satu mesin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b500c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kerangka untuk MirroredStrategy telah disiapkan.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Contoh penggunaan MirroredStrategy (konseptual)\n",
    "# distribution = tf.distribute.MirroredStrategy()\n",
    "#\n",
    "# with distribution.scope():\n",
    "#     # Semua pembuatan model dan kompilasi harus berada di dalam scope strategi\n",
    "#     mirrored_model = keras.models.Sequential([...])\n",
    "#     mirrored_model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "#\n",
    "# # batch_size harus dapat dibagi rata dengan jumlah replika/GPU\n",
    "# # history = mirrored_model.fit(X_train, y_train, epochs=10,\n",
    "# #                                batch_size=32 * distribution.num_replicas_in_sync)\n",
    "print(\"Kerangka untuk MirroredStrategy telah disiapkan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3b605",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. TensorFlow Serving\n",
    "Setelah model dilatih, Anda perlu cara untuk menyebarkannya agar aplikasi lain dapat menggunakannya untuk inferensi. TensorFlow Serving adalah sistem penyajian (serving system) yang fleksibel dan berkinerja tinggi untuk model machine learning, yang dirancang untuk lingkungan produksi.\n",
    "\n",
    "Prosesnya:\n",
    "1. Simpan Model: Simpan model terlatih Anda dalam format SavedModel TensorFlow. Ini adalah format universal yang berisi semua informasi yang diperlukan untuk menyajikan model.\n",
    "2. Jalankan TF Serving: TF Serving dapat dijalankan menggunakan Docker. Anda cukup mengarahkan TF Serving ke direktori tempat SavedModel Anda berada.\n",
    "3. Kirim Permintaan: Aplikasi klien dapat mengirim permintaan inferensi ke TF Serving melalui gRPC atau REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c08761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kerangka untuk deployment dengan TF Serving telah disiapkan.\n"
     ]
    }
   ],
   "source": [
    "# 1. Menyimpan model ke format SavedModel\n",
    "# (Asumsikan 'model' adalah model Keras yang sudah dilatih)\n",
    "# model.save(\"my_fashion_mnist_model\", save_format=\"tf\")\n",
    "\n",
    "# 2. Perintah Docker untuk menjalankan TF Serving (dijalankan di terminal)\n",
    "# docker run -it --rm -p 8500:8500 -p 8501:8501 \\\\\n",
    "#    -v \"$PWD/my_fashion_mnist_model:/models/my_fashion_mnist_model\" \\\\\n",
    "#    -e MODEL_NAME=my_fashion_mnist_model \\\\\n",
    "#    tensorflow/serving\n",
    "\n",
    "# 3. Mengirim permintaan dari klien Python (contoh)\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# X_new = X_test[:3] # Contoh data input\n",
    "# request_json = json.dumps({\"signature_name\": \"serving_default\",\n",
    "#                            \"instances\": X_new.tolist()})\n",
    "# response = requests.post(\"http://localhost:8501/v1/models/my_fashion_mnist_model:predict\",\n",
    "#                          data=request_json)\n",
    "# y_proba = json.loads(response.text)['predictions']\n",
    "print(\"Kerangka untuk deployment dengan TF Serving telah disiapkan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9af80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. TensorFlow Lite (TFLite)\n",
    "Untuk men-deploy model di perangkat seluler atau embedded system, kita memerlukan model yang ringan dan cepat. TensorFlow Lite (TFLite) adalah toolkit untuk mengonversi dan mengoptimalkan model TensorFlow untuk inferensi di perangkat tersebut.\n",
    "\n",
    "Prosesnya:\n",
    "1. Konversi Model: Gunakan TFLiteConverter untuk mengonversi SavedModel atau model Keras menjadi format .tflite yang datar dan efisien.\n",
    "2. Optimisasi (Kuantisasi): Selama konversi, Anda dapat menerapkan teknik optimisasi seperti kuantisasi, yang mengurangi presisi bobot model (misalnya, dari float32 ke int8). Ini secara signifikan mengurangi ukuran model dan mempercepat inferensi, dengan sedikit atau tanpa kehilangan akurasi.\n",
    "3. Jalankan Inferensi: Gunakan interpreter TFLite di aplikasi seluler (Android/iOS) atau perangkat Linux (seperti Raspberry Pi) untuk menjalankan model .tflite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7b770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kerangka untuk konversi ke TensorFlow Lite telah disiapkan.\n"
     ]
    }
   ],
   "source": [
    "# Mengonversi model Keras ke format TFLite\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# Menyimpan model .tflite\n",
    "# with open(\"my_converted_model.tflite\", \"wb\") as f:\n",
    "#     f.write(tflite_model)\n",
    "\n",
    "# Menggunakan kuantisasi untuk optimisasi\n",
    "# converter_quant = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter_quant.optimizations = [tf.lite.Optimize.DEFAULT] # Mengaktifkan kuantisasi\n",
    "# tflite_quantized_model = converter_quant.convert()\n",
    "print(\"Kerangka untuk konversi ke TensorFlow Lite telah disiapkan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec71026c",
   "metadata": {},
   "source": [
    "### 5. TensorFlow.js (TFJS)\n",
    "TensorFlow.js (TFJS) adalah library JavaScript untuk melatih dan men-deploy model Machine Learning langsung di browser atau di Node.js.\n",
    "\n",
    "Manfaat:\n",
    "\n",
    "* Tidak perlu instalasi di sisi pengguna.\n",
    "* Interaktivitas tinggi.\n",
    "* Privasi data (data tidak perlu meninggalkan browser pengguna).\n",
    "\n",
    "Prosesnya:\n",
    "1. Konversi Model: Gunakan command-line tool `tensorflowjs_converter` untuk mengubah `SavedModel` atau model Keras menjadi format yang dapat dibaca oleh TFJS.\n",
    "2. Sajikan di Web: Muat dan jalankan model di aplikasi web Anda menggunakan library TFJS\n",
    "\n",
    "```bash\n",
    "# Perintah konversi (dijalankan di terminal)\n",
    "# tensorflowjs_converter --input_format=keras \\\\\n",
    "#                        ./my_keras_model.h5 ./my_tfjs_model\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
